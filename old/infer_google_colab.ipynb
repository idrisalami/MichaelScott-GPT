{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1679168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "import os\n",
    "os.chdir('/Users/idrishouiralami/Documents/projets_code/GPT')\n",
    "from utils.transformer import build_transformer\n",
    "from gpt_tokenizers.tiktoken import TiktokenTokenizer\n",
    "from utils.masks import Masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e52348",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bebe891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_o): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_o): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "        (cross_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (w_o): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-2): 3 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (src_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(50260, 256)\n",
       "  )\n",
       "  (tgt_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(50260, 256)\n",
       "  )\n",
       "  (src_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.01, inplace=False)\n",
       "  )\n",
       "  (tgt_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.01, inplace=False)\n",
       "  )\n",
       "  (projection_layer): ProjectionLayer(\n",
       "    (proj): Linear(in_features=256, out_features=50260, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Load config\n",
    "with open(\"models/config_tiktoken.json\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "PAD, BOS, EOS = cfg[\"specials\"][\"PAD\"], cfg[\"specials\"][\"BOS\"], cfg[\"specials\"][\"EOS\"]\n",
    "SHIFT = cfg[\"specials\"][\"SHIFT\"]\n",
    "\n",
    "# 2) Rebuild model skeleton with the SAME hyperparams\n",
    "model = build_transformer(\n",
    "    src_vocab_size=cfg[\"src_vocab_size\"],\n",
    "    tgt_vocab_size=cfg[\"tgt_vocab_size\"],\n",
    "    src_seq_len=cfg[\"src_seq_len\"],\n",
    "    tgt_seq_len=cfg[\"tgt_seq_len\"],\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    N=cfg[\"N\"],\n",
    "    h=cfg[\"h\"],\n",
    "    dropout=cfg[\"dropout\"],\n",
    "    d_ff=cfg[\"d_ff\"],\n",
    ")\n",
    "\n",
    "# 3) Load weights; map to CPU (or 'mps' / 'cuda' if available)\n",
    "device = (\"cuda\" if torch.cuda.is_available()\n",
    "          else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "          else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"models/weights_tiktoken.pt\", map_location=device)[\"model\"])\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e91fe1",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3e94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SRC_LEN = 256 # max tokens for dialogue context\n",
    "MAX_TGT_LEN = 128 # max tokens for Michael's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10372cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TiktokenTokenizer()  # SHIFT=3, PAD=0,BOS=1,EOS=2 by default\n",
    "\n",
    "masks = Masks(pad_id=PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2deabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helpers: boolean masks where True = keep, False = pad/blocked ---\n",
    "\n",
    "def pad_to(ids, L, pad=PAD):\n",
    "    return ids[:L] + [pad] * max(0, L - len(ids))\n",
    "\n",
    "def dec_mask_from(dec_in, pad_id=PAD):\n",
    "    # (B,T) -> (B,1,T,T) keep-mask (padding ∧ causal)\n",
    "    B, T = dec_in.size()\n",
    "    tgt_pad = (dec_in != pad_id).unsqueeze(1).unsqueeze(2)               # (B,1,1,T)\n",
    "    tgt_causal = torch.tril(torch.ones(T, T, dtype=torch.bool, device=dec_in.device)).unsqueeze(0).unsqueeze(1)  # (1,1,T,T)\n",
    "    return tgt_pad & tgt_causal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd1805",
   "metadata": {},
   "source": [
    "### Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b083d3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICHAEL] I'm not going to be a little bit of a little bit of you.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def generate_reply(context_text, max_new_tokens=80):\n",
    "    model.eval()\n",
    "    # encode & pad the source once\n",
    "    src_ids = pad_to(tok.encode(context_text), MAX_SRC_LEN)\n",
    "    src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)   # (1,S)\n",
    "    src_mask = masks.encoder(src)                                                # (1,1,1,S)\n",
    "    enc_out = model.encode(src, src_mask)                                       # (1,S,d)\n",
    "\n",
    "    # start decoder with BOS\n",
    "    dec = torch.tensor([[BOS]], dtype=torch.long, device=device)                # (1,1)\n",
    "\n",
    "    # generate until EOS or max_new_tokens reached\n",
    "    while (dec.size(1) - 1) < max_new_tokens:\n",
    "        tgt_mask = dec_mask_from(dec)                                           # (1,1,T,T)\n",
    "        dec_out = model.decode(enc_out, src_mask, dec, tgt_mask)                # (1,T,d)\n",
    "        logits  = model.project(dec_out)                                        # (1,T,V)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)              # greedy → (1,1)\n",
    "        dec = torch.cat([dec, next_token], dim=1)                               # append\n",
    "        if next_token.item() == EOS:\n",
    "            break\n",
    "\n",
    "    return tok.decode(dec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08b53026",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_reply_topk(context_text, max_new_tokens=80, temperature=0.8, top_k=50, top_p=0.9):\n",
    "    model.eval()\n",
    "    # encode & pad the source once\n",
    "    src_ids = pad_to(tok.encode(context_text), MAX_SRC_LEN)\n",
    "    src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    src_mask = masks.encoder(src)     \n",
    "    enc_out = model.encode(src, src_mask)\n",
    "\n",
    "    # start decoder with BOS\n",
    "    dec = torch.tensor([[BOS]], dtype=torch.long, device=device)\n",
    "\n",
    "    # generate until EOS or max_new_tokens reached\n",
    "    while (dec.size(1) - 1) < max_new_tokens:\n",
    "        tgt_mask = dec_mask_from(dec)\n",
    "        dec_out = model.decode(enc_out, src_mask, dec, tgt_mask)\n",
    "        logits = model.project(dec_out)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Top-k filtering\n",
    "        if top_k > 0:\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = -float('inf')\n",
    "        \n",
    "        # Top-p (nucleus) filtering\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = -float('inf')\n",
    "        \n",
    "        # Sample from the filtered distribution\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "        \n",
    "        dec = torch.cat([dec, next_token], dim=1)\n",
    "        if next_token.item() == EOS:\n",
    "            break\n",
    "\n",
    "    return tok.decode(dec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b15d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICHAEL] I'm not going to be a little bit of a little bit of you.\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(generate_reply(\"[JIM] Is Dwight the assistant regional manager?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66e1f1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICHAEL] I don't want to do that.\n"
     ]
    }
   ],
   "source": [
    "print(generate_reply_topk(\"[JIM] Hey, how are you?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
