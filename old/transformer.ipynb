{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f3dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbcba6",
   "metadata": {},
   "source": [
    "https://medium.com/@bavalpreetsinghh/transformer-from-scratch-using-pytorch-28a5d1b2e033"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce3ced",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer.webp\" alt=\"Transformer\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c7b48",
   "metadata": {},
   "source": [
    "# Creating the fundamental blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489f42c",
   "metadata": {},
   "source": [
    "## Input Embedding\n",
    "\n",
    "It allows to convert the original sentence into a vector of X dimensions (the original Transformer model uses 512 as a size of dimension d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053547b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/embedding.webp\" alt=\"Embedding\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2356bc6",
   "metadata": {},
   "source": [
    "Purpose of the `__init__()` method\n",
    "- initialize the state of an object (i.e. set up initial values)\n",
    "- define the layers and components that the neural network will use\n",
    "- ensure that any necessary setup or initialization code is executed when an object is created\n",
    "\n",
    "`super()` function is used to call a method from the parent class, in this case, calls the `__init__()` method of the nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca9290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings acording to the paper\n",
    "        return self.embedding(x) * mt.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6ebf3",
   "metadata": {},
   "source": [
    "- `nn.Embedding(vocab_size, d_model)`: this creates an embedding layer that maps indices to a d_model-dimensional vector. The embedding layer is initialied randomly and these vectors are learned during training.\n",
    "- `self.embedding(x)`: Here, x is a tensor of token indices. The mebedding layer looks up to the vector for each token index in x.\n",
    "- `math.sqrt(self.d_model)`: done to maintain variance of the embeddings, helping with training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fc51c",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Helps the model understand the position of each word in a sentence, since transformers do not inherently process tokens in a sequential manner like RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab1bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-mt.log(10000.0) / d_model))\n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cos to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81415360",
   "metadata": {},
   "source": [
    "- `pe = pe.unsqueeze(0)`: because there will be batch of sentences, we need to add the batch dimension to the tensor (seq_len, d_model) -> (1, seq_len, d_model).\n",
    "- `self.register_buffer('pe', pe)`: we store pe without making it a learnable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b04a9",
   "metadata": {},
   "source": [
    "`forward` method:\n",
    "- we add positional encodings to the input embeddings x and applies dropout\n",
    "- we tell the model not to learn pos encodings as they are fixed, using `required_grad_(False)` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f42ec",
   "metadata": {},
   "source": [
    "## Projection Layer\n",
    "\n",
    "Used to convert the high-dimensional vectors (output of the decoder) into logits over the vocabulary (typically the last layer in the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65fffd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36c9e9",
   "metadata": {},
   "source": [
    "`self.proj`: an instance of `nn.Linear` that maps from d_model dimensions to vocab_size dimensions. This tensor can then be used to compute the probability distribution over the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091d069",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Technique used to improve the training of NNs by normalizing inputs across the features for each training example.\n",
    "(The dimensions of the word embeddings (e.g. 512 dimensions) are referred to as \"features\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3bd362",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/LayerNorm.webp\" alt=\"Embedding\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6586c5",
   "metadata": {},
   "source": [
    "Both Gamma & Beta are learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e61428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps: float=1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (batch, seq_len, hidden_size)\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e347476",
   "metadata": {},
   "source": [
    "- Initialize with a small epsilon value eps to prevent division by zero\n",
    "- `nn.Parameter` makes gamma & beta learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392f37b",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "Used to help with the training of deep neural networks by allowing gradients to flow more easily through the network. See this paper for more details: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385 \"He et al., 2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc3fde",
   "metadata": {},
   "source": [
    "`ResidualConnection` together with `LayerNormalization` form the `Add & Norm` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496dee7",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "This helps in adding non-linearity to the model, allowing it to learn more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a85cc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len,d_model) --> (batcg_size, seq_len, d_ff) --> (batch_size, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fa01f",
   "metadata": {},
   "source": [
    "- We initialize d_model, d_ff (dimension of the feed froward layer), and dropout\n",
    "- We define two linear models: `linear_1` (W1, b1) and `linear_2` (W2, b2) and the ReLU to introduce non-linearities into the model\n",
    "- The dropout is used to prevent overfitting by randomly setting a fraction of the input units to zero during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee8eff",
   "metadata": {},
   "source": [
    "## Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553df133",
   "metadata": {},
   "source": [
    "### 1. Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da69f68",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/attention.webp\" alt=\"Embedding\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753980a",
   "metadata": {},
   "source": [
    "### 2. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf42d3",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/multi-head.webp\" alt=\"Embedding\" width=\"700\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52967aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\" # To make sure d_model is divisible by the number of heads\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Applying the formula from the paper\n",
    "        attention_scores = (query @ key.transpose(-2, -1) / mt.sqrt(d_k))\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(query.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine all the head together\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbead0",
   "metadata": {},
   "source": [
    "`__init__`method:\n",
    "- Initialize $d_{model}$, $h$ (number of attention heads) and $dropout$ rate\n",
    "- Ensure $d_{model}$ is divisible by $h$\n",
    "- Defines linear layers for the query $W_q$, key $W_k$, value $W_v$, and output $W_o$\n",
    "\n",
    "`attention` static method:\n",
    "- Computes the attention formula\n",
    "- Applies mask / dropout if provided\n",
    "\n",
    "`forward` method:\n",
    "- We apply linear transformations to the input tensors q, k and v using learnable weight matrices $W_q$, $W_k$ and $W_v$\n",
    "- We reshape the tensors to have a separate dimension for the number of heads, the dimension becomes (batch, seq_len, h, d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db314f3",
   "metadata": {},
   "source": [
    "### What is a Mask ?\n",
    "\n",
    "- **Padding Mask**: is used to ensure that padding tokens in the input sequence do not influence the attention mechanism (when batching sequences of different lengths we need all examples in a batch to have the same length, so we pad the shorter ones with a special token — usually called `<PAD>` or token ID `0`). Used in both Encoded & Decoder blocks.\n",
    "\n",
    "- **Look-Ahead Mask (Causal Mask)**: is used to ensure that during training and inference, each position in the output sequence can only attend the positions before it and the current positions, but not any future posiitions. Used in Decoder block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60460ff5",
   "metadata": {},
   "source": [
    "# Encoder Block\n",
    "\n",
    "Contains one multi-head attention, two Add and Norm & one feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b897bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, features:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6bf84",
   "metadata": {},
   "source": [
    "- Here, the `self_attention_block` takes the input (x, x, x) as the (query, key, value), which is why it's called 'self-attention'\n",
    "- Why `lambda x: self.self_attention_block(x, x, x, src_mask)`?\n",
    "  - The `ResidualConnection` expects a **function** as its second argument (`sublayer`), it will call it internally as `sublayer(self.norm(x))`\n",
    "  - If we just passed `self.self_attention_block` directly, Python would complain because that function needs 4 arguments `(q, k, v, mask)` — but `ResidualConnection` will only call it with (`x`)\n",
    "  - So we wrap it in a **lambda** to “freeze” the extra arguments and call the attention block with (x, x, x, src_mask)\n",
    "- Why `self.feed_forward_block` (and not `self.feed_forward_block.forward`)?\n",
    "  - feed_forward_block is an instance of a PyTorch nn.Module (your class FeedForwardBlock)\n",
    "  - In PyTorch, every nn.Module has a special behavior: calling the module automatically invokes its forward() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645be00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features:int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67f5ee",
   "metadata": {},
   "source": [
    "We create this `Encoder` class because an encoder is most of the time made of N identical `EncoderBlock` layers (each with its own learnable weights), to provide more depth to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a93bc8",
   "metadata": {},
   "source": [
    "# Decoder Block\n",
    "\n",
    "Contains a self-attention mechanism, a cross attention mechanism (attenting to the encoder's output), and a feed-forward network, all surrounded by residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, features:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8175412",
   "metadata": {},
   "source": [
    "- `src_mask`: source mask to prevent the model from attending to padding tokens in the source input\n",
    "- `tgt_mask`: target mask to prevent the model froom attending to future tokens in the target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af339d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3212d",
   "metadata": {},
   "source": [
    "We create this `Decoder` class because a decoder is most of the time made of N identical `DecoderBlock` layers (each with its own learnable weights), to provide more depth to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6893c2",
   "metadata": {},
   "source": [
    "# Transformer Class\n",
    "\n",
    "Encapsulates the entire transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e67700",
   "metadata": {},
   "source": [
    "`encode` method:\n",
    "- apply source embeddings to the input tensor $src$\n",
    "- add positional encodings to the embedded source tensor\n",
    "- pass the resulting tensor through the encoder along with the source mask (to handle padding tokens)\n",
    "- returns the encoded representation of the source sequence of shape (batch_size, seq_len, d_model)\n",
    "\n",
    "`decode` method:\n",
    "- apply target embeddings to the input tensor tgt\n",
    "- add positional encodings to the embedded target tensor\n",
    "- pass the resulting tensor through the decoder along with the encoder output and the respective masks\n",
    "- returns the decoded representation of the target sequence of shape (batch_size, seq_len, d_model)\n",
    "\n",
    "`project` method:\n",
    "  - apply the projection layer to map the d_model output to vocab_size dimensional logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size:int, src_seq_len:int, tgt_seq_len:int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encodings\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create the encoder block\n",
    "    encoder_blocks = []\n",
    "    for _ in range (N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, d_model, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder block\n",
    "    decoder_blocks = []\n",
    "    for _ in range (N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, d_model, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    \n",
    "    return transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
