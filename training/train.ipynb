{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8684e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/idrishouiralami/Documents/projets_code/GPT')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.masks import Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c14b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr: float = 3e-4\n",
    "    betas: tuple[float, float] = (0.9, 0.95)\n",
    "    weight_decay: float = 0.05\n",
    "    clip_grad_norm: float = 1.0\n",
    "    pad_id: int = 0\n",
    "    epochs: int = 10\n",
    "    amp: bool = False  # set True on CUDA if you want mixed precision\n",
    "    log_progress: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, device: Optional[str] = None, cfg: Optional[TrainConfig] = None):\n",
    "        self.model = model\n",
    "        self.device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.cfg = cfg or TrainConfig()\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=self.cfg.pad_id)\n",
    "        self.opt = AdamW(self.model.parameters(),\n",
    "                         lr=self.cfg.lr,\n",
    "                         betas=self.cfg.betas,\n",
    "                         weight_decay=self.cfg.weight_decay)\n",
    "        self.masks = Masks(pad_id=self.cfg.pad_id)\n",
    "        self.scaler = torch.amp.GradScaler('cuda', enabled=self.cfg.amp)\n",
    "\n",
    "    def _run_epoch(self, loader, train: bool) -> float:\n",
    "        self.model.train(train)\n",
    "        total, steps = 0.0, 0\n",
    "\n",
    "        pbar = tqdm(loader, desc=(\"train epoch\" if train else \"val epoch\"),\n",
    "                    leave=False, disable=not self.cfg.log_progress)\n",
    "\n",
    "        for src, dec_in, labels in pbar:\n",
    "            src, dec_in, labels = src.to(self.device), dec_in.to(self.device), labels.to(self.device)\n",
    "\n",
    "            # masks\n",
    "            src_mask = self.masks.encoder(src)\n",
    "            _, _, tgt_mask = self.masks.decoder(dec_in)\n",
    "\n",
    "            # forward (+ AMP if enabled)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=self.cfg.amp):\n",
    "                enc_out = self.model.encode(src, src_mask)\n",
    "                dec_out = self.model.decode(enc_out, src_mask, dec_in, tgt_mask)\n",
    "                logits = self.model.project(dec_out)             # (B, T-1, V)\n",
    "                loss = self.crit(logits.reshape(-1, logits.size(-1)),\n",
    "                                 labels.reshape(-1))             # use reshape, not view\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(set_to_none=True)\n",
    "                if self.cfg.amp:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.clip_grad_norm)\n",
    "                    self.scaler.step(self.opt)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.clip_grad_norm)\n",
    "                    self.opt.step()\n",
    "\n",
    "            total += loss.item()\n",
    "            steps += 1\n",
    "            if self.cfg.log_progress:\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.3f}\"})\n",
    "\n",
    "        pbar.close()\n",
    "        return total / max(1, steps)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader) -> float:\n",
    "        return self._run_epoch(loader, train=False)\n",
    "\n",
    "    def fit(self, train_loader, val_loader=None):\n",
    "        history = []\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            tr = self._run_epoch(train_loader, train=True)\n",
    "            if val_loader is not None:\n",
    "                va = self.evaluate(val_loader)\n",
    "                ppl = math.exp(va)\n",
    "                print(f\"epoch {epoch+1:02d} | train {tr:.3f} | val {va:.3f} | ppl {ppl:.2f}\")\n",
    "                history.append({\"epoch\": epoch+1, \"train\": tr, \"val\": va, \"ppl\": ppl})\n",
    "            else:\n",
    "                print(f\"epoch {epoch+1:02d} | train {tr:.3f}\")\n",
    "                history.append({\"epoch\": epoch+1, \"train\": tr})\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c358e81",
   "metadata": {},
   "source": [
    "**`for src, dec_in, labels in pbar`** batch loop:\n",
    "- `src`: source sequence (the dialogue before Michael speaks)\n",
    "- `dec_in`: decoder input (shifted Michael response)\n",
    "- `labels`: expected next tokens\n",
    "- `src_mask` &  `tgt_mask`: builds proper masks for src & tgt\n",
    "- `enc_out`: encode the src sequence\n",
    "- `dec_out`: decode given src sequence & previous outputs\n",
    "- `logits`: projects to vocab size\n",
    "- `loss`: computes the CrossEntropyLoss\n",
    "- `if train`: then backpropagates the weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
