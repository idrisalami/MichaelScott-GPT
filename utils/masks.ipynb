{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af302e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Dict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fe8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masks:\n",
    "    \"\"\"\n",
    "    Utilities to build boolean attention masks for Transformer models.\n",
    "    Returns masks where True = keep/attend, False = masked-out.\n",
    "\n",
    "    Usage:\n",
    "        m = Masks(pad_id=0)\n",
    "        src_mask = m.encoder(src)                     # (B,1,1,S)\n",
    "        tgt_pad, tgt_caus, tgt_mask = m.decoder(tgt)  # (B,1,1,T), (1,1,T,T), (B,1,T,T)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_id: int) -> None:\n",
    "        self.pad_id = pad_id\n",
    "        # cache for causal masks keyed by (T, device)\n",
    "        self._causal_cache: Dict[tuple[int, torch.device], torch.Tensor] = {}\n",
    "\n",
    "    # ---------- encoder ----------\n",
    "    def encoder(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        src: LongTensor (B, S)\n",
    "        returns: BoolTensor (B, 1, 1, S)\n",
    "        \"\"\"\n",
    "        if src.dim() != 2:\n",
    "            raise ValueError(f\"encoder mask expects src of shape (B,S), got {tuple(src.shape)}\")\n",
    "        return (src != self.pad_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # ---------- decoder ----------\n",
    "    def decoder(self, tgt_in: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        tgt_in: LongTensor (B, T)\n",
    "        returns:\n",
    "            tgt_pad   (B, 1, 1, T)  True at non-PAD\n",
    "            tgt_causal  (1, 1, T, T)  True where i >= j\n",
    "            tgt_mask  (B, 1, T, T)  tgt_pad AND tgt_caus\n",
    "        \"\"\"\n",
    "        if tgt_in.dim() != 2:\n",
    "            raise ValueError(f\"decoder mask expects tgt_in of shape (B,T), got {tuple(tgt_in.shape)}\")\n",
    "        B, T = tgt_in.shape\n",
    "        device = tgt_in.device\n",
    "\n",
    "        tgt_pad = (tgt_in != self.pad_id).unsqueeze(1).unsqueeze(2)   # (B,1,1,T)\n",
    "        tgt_causal = self._causal(T, device)                            # (1,1,T,T)\n",
    "        tgt_mask = tgt_pad & tgt_causal                                 # (B,1,T,T)\n",
    "        return tgt_pad, tgt_causal, tgt_mask\n",
    "\n",
    "    # ---------- internals ----------\n",
    "    def _causal(self, T: int, device: Optional[torch.device] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Build or fetch a cached lower-triangular BoolTensor (1,1,T,T).\n",
    "        \"\"\"\n",
    "        if T < 1:\n",
    "            raise ValueError(\"T must be >= 1\")\n",
    "        key = (T, device or torch.device(\"cpu\"))\n",
    "        m = self._causal_cache.get(key)\n",
    "        if m is None:\n",
    "            m = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device)).unsqueeze(0).unsqueeze(1)\n",
    "            self._causal_cache[key] = m\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fc8c0",
   "metadata": {},
   "source": [
    "**`Masks.encoder`**:\n",
    "- `src`: input token of shape (batch_size, seq_len)\n",
    "- `(src != pad_id)`: boolean tensor — False when the token is a padding token PAD, else True\n",
    "- `unsqueeze(1).unsqueeze(2)`: add two singleton dimensions(B, 1, 1, S), to match the attention score shape (B, heads, T, S)\n",
    "\n",
    "**`Masks.decoder`**:\n",
    "- `tgt_in`: decoder input of shape (B, T)\n",
    "- `tgt_pad`: same idea as before, marks non-PAD positions → shape (B,1,1,T)\n",
    "- `tgt_causal`: a lower-triangular matrix of Trues → (1,1,T,T), ensures that token $t$ can only attend $\\le t$ (causal/self-attention mask)\n",
    "- `tgt_mask = tgt_pad & tgt_causal`: combine both → (B,1,T,T), only keeps real tokens in the past or present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e038c9",
   "metadata": {},
   "source": [
    "**Why cache the causal mask?**\n",
    "\n",
    "In Transformer decoders, the causal mask is a lower-triangular matrix `(1, 1, T, T)` that prevents each token from attending to future positions.  \n",
    "Since this mask only depends on the sequence length `T` and the device (CPU/GPU), it remains the same across all batches during training.\n",
    "\n",
    "Recomputing it every forward pass (e.g., `torch.tril(torch.ones(...))`) causes:\n",
    "- unnecessary tensor allocations on the device,  \n",
    "- repeated memory transfers,  \n",
    "- and a noticeable slowdown for large sequence lengths.\n",
    "\n",
    "By caching one mask per `(T, device)` pair, we:\n",
    "- create it once,  \n",
    "- reuse it efficiently across all steps,  \n",
    "- and eliminate redundant computation.\n",
    "\n",
    "Result: cleaner code and faster training — especially for long sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
