{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a722e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c28f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfficeSeq2Seq(Dataset):\n",
    "    \"\"\"Seq2Seq dataset that pads inside __getitem__ and does teacher forcing shift.\"\"\"\n",
    "    def __init__(self,\n",
    "                 enc_src: List[List[int]],\n",
    "                 enc_tgt: List[List[int]],\n",
    "                 max_src: int,\n",
    "                 max_tgt: int,\n",
    "                 pad_id: int) -> None:\n",
    "        super().__init__()\n",
    "        assert len(enc_src) == len(enc_tgt), \"src/tgt length mismatch\"\n",
    "        self.src = enc_src\n",
    "        self.tgt = enc_tgt\n",
    "        self.max_src = max_src\n",
    "        self.max_tgt = max_tgt\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_to(ids: List[int], L: int, pad_id: int) -> List[int]:\n",
    "        if len(ids) >= L:\n",
    "            return ids[:L]\n",
    "        return ids + [pad_id] * (L - len(ids))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # pad source/target\n",
    "        s = self._pad_to(self.src[i], self.max_src, self.pad_id)        # (S,)\n",
    "        t = self._pad_to(self.tgt[i], self.max_tgt, self.pad_id)        # (T,)\n",
    "\n",
    "        # teacher forcing shift\n",
    "        dec_in = t[:-1]     # (T-1,)\n",
    "        labels = t[1:]      # (T-1,)\n",
    "\n",
    "        # convert to long & contiguous tensors\n",
    "        s = torch.tensor(s, dtype=torch.long).contiguous()\n",
    "        dec_in = torch.tensor(dec_in, dtype=torch.long).contiguous()\n",
    "        labels = torch.tensor(labels, dtype=torch.long).contiguous()\n",
    "        return s, dec_in, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ebbc3",
   "metadata": {},
   "source": [
    "`__len__`: tells PyTorch how many samples are in the dataset\n",
    "\n",
    "`__getitem__`: when DataLoader asks for the i-th item in our dataset:\n",
    "- take the i-th source (self.src[i]) and pad it to max_src\n",
    "- take the i-th target (self.tgt[i]) and pad it to max_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214292da",
   "metadata": {},
   "source": [
    "**Teacher Forcing & Sequence Shifting**:\n",
    "\n",
    "In seq2seq models (like Transformers), we train the decoder to predict the next token\n",
    "given all previous true tokens, not its own predictions.\n",
    "This is called Teacher Forcing.\n",
    "\n",
    "We prepare the target sequence `t` like this:\n",
    "\n",
    "| Token role | Example | Explanation |\n",
    "|-------------|----------|-------------|\n",
    "| `t` | `[BOS, H, e, l, l, o, EOS]` | the full target sequence |\n",
    "| Decoder input `dec_in` | `[BOS, H, e, l, l, o]` | shifted right (starts with BOS) |\n",
    "| Labels `labels` | `[H, e, l, l, o, EOS]` | shifted left (the \"next\" tokens) |\n",
    "\n",
    "During training, at each time step *t*, the model sees the real previous token (from `dec_in[t-1]`) and learns to predict the next token (`labels[t]`).\n",
    "\n",
    "At inference time, we feed back the modelâ€™s *own* predictions instead\n",
    "(one token at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33545ac5",
   "metadata": {},
   "source": [
    "**`pad_to`** (paddling function):\n",
    "- if the list ids (token sequence) is longer than $L$, it will truncate it: `ids[:L]`\n",
    "- if shorter, append enough [pad] tokens to reach length L"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
